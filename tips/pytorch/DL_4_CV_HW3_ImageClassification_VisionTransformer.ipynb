{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcNL_2-Avpq8"
      },
      "source": [
        "# Instruction\n",
        "\n",
        "\n",
        "*   Feel free to modify the skeleton code as needed. It is provided solely as a framework for your solution.\n",
        "*   You must submit this notebook in both .html and .ipynb formats.\n",
        "*   Ensure to change your runtime to GPU for faster training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdT85aqzwDQL"
      },
      "source": [
        "#Objectives\n",
        "\n",
        "In this assignment, you will use vision transformer for image classification on MNIST dataset:\n",
        "\n",
        "Most of the code for this homework has been provided to you in this notebook and you should be able to train the model as it is to see how it works.\n",
        "\n",
        "You need to do these tasks:\n",
        "\n",
        "\n",
        "1.   **Patch Extraction**: Run *img_to_patch* on a batch of images from train_dataloader with both *flatten_channels=True* and *flatten_channels=False* and *patch_size=4*, then discuss about shape of the output: What does each dimension show?\n",
        "2.   **Training**: Train the model for 10 epochs and show learning curves.\n",
        "3.   **Visualization of cosine similarity of positional embeddings**: you need to extract positional embeddings from the model before and after training, calculate the cosine similarity between all pairs of positional embeddings, and visualize the results. You can use *torch.nn.functional.cosine_similarity* for this task.\n",
        "\n",
        "4.\t**Visualization of attention maps**: your task is to modify the TransformerBlock class/ and ViT model such that it returns the attention weights from the self_attention layer during the forward pass. You will then visualize the attention maps for the last layer which are averaged over the heads. (You need to train the model again after making these changes.)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bUkqzS6yvMr"
      },
      "source": [
        "#Importing Required Libraries\n",
        "\n",
        "Let's start by importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9e4572JvHtM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# For repeatable experiments\n",
        "random_seed = 32\n",
        "torch.manual_seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCYGVCGw7bLA"
      },
      "source": [
        "# Loading the Dataset and creating dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IFMtChZyyQc"
      },
      "source": [
        "### Download Dataset\n",
        "\n",
        "In this assignment, you'll be working on MNIST Dataset which contains over 60,000 images of handwritten digits.\n",
        "The first step is to download the dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0GlXaJocy6km"
      },
      "outputs": [],
      "source": [
        "# Downloading training data for MNIST dataset\n",
        "train_dataset = MNIST(root=\"dataset/\",\n",
        "                      train=True,\n",
        "                      download=True,\n",
        "                      transform=transforms.Compose([\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.1307,), (0.3081,))\n",
        "                      ]))\n",
        "\n",
        "# Downloading testing data for MNIST dataset\n",
        "test_dataset = MNIST(root=\"dataset/\",\n",
        "                     train=False,\n",
        "                     download=True,\n",
        "                     transform=transforms.Compose([\n",
        "                     transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.1307,), (0.3081,))\n",
        "                     ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vcOaj5qy7JO"
      },
      "source": [
        "### Create DataLoader\n",
        "\n",
        "Now, let's define our dataloader to pass samples in **minibatches**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYMs37BWzE_u"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 64\n",
        "test_batch_size=128\n",
        "\n",
        "# Create data loaders to iterate over data\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training data size: {len(train_loader.dataset)}\")\n",
        "print(f\"Test data size: {len(test_loader.dataset)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbfITs6G5mzy"
      },
      "source": [
        "Let's check size of each batch in our train_loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvvSQnJV5Z-X"
      },
      "outputs": [],
      "source": [
        "# Displaying the shape of a single batch from the train loader\n",
        "train_examples = enumerate(train_loader)\n",
        "batch_idx, (train_images, train_labels) = next(train_examples)\n",
        "\n",
        "print(f\"Shape of images in train loader [B, C, H, W]: {train_images.shape}\")\n",
        "print(f\"Shape of labels in train loader [B]: {train_labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_cbW9qszHgT"
      },
      "source": [
        "### Visualizing Sample Images\n",
        "\n",
        "Let's visualize some of sample images from the train_dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jq9MZEvzK24"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "    plt.subplot(2, 3, i + 1)\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(train_images[i][0], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Label: {}\".format(train_labels[i]))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MloPyIsxKrUT"
      },
      "source": [
        "# Vision Transformer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XtqbsvTVmg8"
      },
      "source": [
        "### Patch Extraction\n",
        "\n",
        "Patch extraction is the first step in Vision Transformer (ViT) architecture, it divides the input image into smaller, non-overlapping patches that can be processed independently.\n",
        "Here, we defined a function that divides images into patches:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2hnZbBVV5_r1"
      },
      "outputs": [],
      "source": [
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "        flatten_channels - If True, the patches will be returned in a flattened format\n",
        "                           as a feature vector instead of a image grid.\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
        "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SkctwPsm8wI"
      },
      "source": [
        "Run this function with **flatten_channels=True** and **flatten_channels=False** on the same batch from the previous cells, set **batch_size=4** and discuss the shape of the output: what each dimension corresponds to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQLdKIKw-QD5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "# Your code here\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DEJfm1MomVf"
      },
      "source": [
        "Notice how setting flatten_channels affects the shape of the output.\n",
        "\n",
        "Now, let's visualize the patches to see how they look. In this case, we will use the output of the **img_to_patch** function with **flatten_channels=False**.\n",
        "We will use **torchvision.utils.make_grid** to visualize the patches as a grid. The **plot_patches** function allows you to control the display by setting **seq=False** to view the patches in their original image shape or **seq=True** to display the image as a sequence of patches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sUiyi7euyB8c"
      },
      "outputs": [],
      "source": [
        "def plot_patches(patches, seq=False):\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(14, 3))\n",
        "\n",
        "  # set nrow: Number of images displayed in each row of the grid\n",
        "  if seq == False:\n",
        "    nrow=int(train_images.shape[2] / patch_size)\n",
        "  else:\n",
        "    nrow =int((train_images.shape[2] / patch_size)**2)\n",
        "\n",
        "  img_grid = make_grid(patches, nrow=nrow , normalize=True, pad_value=0.9)\n",
        "  img_grid = img_grid.permute(1, 2, 0)  # Convert from CxHxW to HxWxC for imshow\n",
        "\n",
        "  ax.imshow(img_grid, cmap='gray')\n",
        "  ax.axis('off')\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv9XKpLkyOpr"
      },
      "source": [
        "You need to plot the extracted patches for the first image in the batch, with **batch_size=4**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiYcdlxOyYYT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "# Your code here\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDggKw5DvzXR"
      },
      "source": [
        "By looking at each individual patch, you can see that recognizing the digits is much harder compared to seeing the whole image. Yet, this is the input we give to the Transformer for digit classification. The model must figure out how to piece these patches together on its own to accurately identify the digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbEacxvI-nwu"
      },
      "source": [
        "### Tranformer Block\n",
        "\n",
        "\n",
        "In Vision Transformers, after the image is divided into patches during patch extraction, each patch is flattened and converted into a token through embedding. These token embeddings are then processed by the transformer block, which is key to learning the relationships between different regions of the image.\n",
        "\n",
        "The Transformer block consists of two main components:\n",
        "\n",
        "\n",
        "*   **Multi-head self-attention**: which allows the model to focus on different patches simultaneously and,\n",
        "*   **Feedforward neural network (MLP)**: for further transformation.\n",
        "\n",
        "Layer normalization is applied before each mechanism to ensure stability and enhance learning efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xEw-wJ9J9Xc4"
      },
      "outputs": [],
      "source": [
        "# Transformer Block with Self-Attention and MLP\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_dim=128, num_heads=4):\n",
        "        # Initialize the parent nn.Module\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # First Layer Normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads,\n",
        "                                                    batch_first=True, dropout=0.1)\n",
        "\n",
        "        # Second Layer Normalization\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Feed-forward network (MLP) with hidden layer and activation\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim),\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # First Layer Normalization\n",
        "        norm_x = self.layer_norm1(x)\n",
        "\n",
        "        #self-attention with residual connection\n",
        "        attn_output= self.self_attention(norm_x, norm_x, norm_x)[0]\n",
        "        x = x + attn_output\n",
        "\n",
        "        # Second Layer Normalization\n",
        "        norm_x=self.layer_norm2(x)\n",
        "\n",
        "        #feed-forward network with residual connection\n",
        "        feed_forward_output = self.feed_forward(norm_x)\n",
        "        x = x + feed_forward_output\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zslYjFad04h8"
      },
      "source": [
        "### ViT Model\n",
        "\n",
        "The vision transformer architecture for an image classification task follows these steps:\n",
        "\n",
        "\n",
        "*   **Patch Extraction**: Split the image into fixed-size patches and flatten them.\n",
        "*   **Patch Embedding**: Apply linear embeddings to the flattened patches (project them to the hidden size dimension).\n",
        "*   **Add positional embeddings**: Add positional embeddings to patch embeddings.\n",
        "*   **Concatenate output token**: Append the output token to the sequence of patch embeddings.\n",
        "*   **Transformer block processing**: Pass the sequence of embeddings through a transformer encoder.\n",
        "*   **Classification**: Use the output token embedding for the final classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8oHHnLhz9ksX"
      },
      "outputs": [],
      "source": [
        "# Vision Transformer (ViT) with Transformer Blocks\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size, in_channels, num_classes, patch_size, hidden_size, num_layers, num_heads=8):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        # Store patch dimension\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Linear layer to embed image patches into the hidden dimension\n",
        "        self.patch_embed = nn.Linear(in_channels * patch_size * patch_size, hidden_size)\n",
        "\n",
        "        # Stack of Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(hidden_size, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Linear layer to output class predictions\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        # Learnable output token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
        "\n",
        "        # Positional embedding to maintain spatial information\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, hidden_size) * 0.001)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Convert image to patches and flatten\n",
        "        patch_seq = img_to_patch(x, self.patch_size, flatten_channels=True)\n",
        "\n",
        "        # Embed patches into the hidden dimension\n",
        "        patch_embeddings = self.patch_embed(patch_seq)\n",
        "\n",
        "        # Add positional embeddings to the patch embeddings\n",
        "        patch_embeddings += self.pos_embed\n",
        "\n",
        "        # Concatenate class token to the patch embeddings\n",
        "        cls_token = self.cls_token.expand(batch_size, 1, -1)\n",
        "        embeddings = torch.cat((cls_token, patch_embeddings), dim=1)\n",
        "\n",
        "        # Pass the embeddings through transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            embeddings = block(embeddings)\n",
        "\n",
        "        # Classification based on the class token output\n",
        "        return self.classifier(embeddings[:, 0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4-5-rJF7TJ5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq8rYS8PzMzj"
      },
      "source": [
        "### Device Configuration for Training\n",
        "\n",
        "Ensure that you have set your device to GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1B3IoatzOSO"
      },
      "outputs": [],
      "source": [
        "# Get device for training.\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhPkRULC3OxY"
      },
      "source": [
        "Let's check the model's structure:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7vEAqWFfq9B"
      },
      "outputs": [],
      "source": [
        "patch_size=4\n",
        "num_classes = 10\n",
        "\n",
        "# Let's define our model:\n",
        "model = ViT(img_size=train_images.shape[2],\n",
        "            in_channels=train_images.shape[1],\n",
        "            num_classes = num_classes,\n",
        "            patch_size=patch_size,\n",
        "            hidden_size=128,\n",
        "            num_layers=8,\n",
        "            num_heads=8).to(device)\n",
        "\n",
        "# View the model's architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPcwVvbPzhLa"
      },
      "source": [
        "Before we start training, let's check the shape of model's output when fed a single batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bibPnuEMlcw"
      },
      "outputs": [],
      "source": [
        "train_examples = enumerate(train_loader)\n",
        "batch_idx, (train_images, train_labels) = next(train_examples)\n",
        "\n",
        "# Pass image through network\n",
        "out= model(train_images.to(device))\n",
        "# Check input and output's shapes\n",
        "print(f\"Shape of images in train loader [B, C, H, W]: {train_images.shape}\")\n",
        "print(f\"Shape of images in train loader [B, num_classes]: {out.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHvyeEd_H7i6"
      },
      "source": [
        "### Train Function\n",
        "\n",
        "Here, we define our training function that is used to train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P1TuIwqQH8XH"
      },
      "outputs": [],
      "source": [
        "# Let's define our training function\n",
        "def train(dataloader, model, loss_fn, optimizer,epoch,train_losses,train_acc):\n",
        "\n",
        "    model.train()\n",
        "    training_loss=0\n",
        "    correct = 0\n",
        "    for Image, Label in tqdm(dataloader):\n",
        "\n",
        "\n",
        "\n",
        "        Image=Image.to(device)\n",
        "        Label=Label.to(device)\n",
        "\n",
        "        pred=model(Image)\n",
        "\n",
        "        loss=loss_fn(pred,Label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "        correct += (pred.argmax(1) == Label).type(torch.float).sum().item()\n",
        "\n",
        "\n",
        "    # Average loss and accuracy for the epoch\n",
        "    training_loss /= len(dataloader)\n",
        "    train_accuracy= (100*correct/len(dataloader.dataset))\n",
        "    train_losses.append(training_loss)\n",
        "    train_acc.append(train_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} \\n Train Accuracy: {train_accuracy:.1f}%, Train loss: {training_loss:.6f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrXP-8loIAWB"
      },
      "source": [
        "### Test Function\n",
        "\n",
        "The test function evaluates the model's predictive performance using the **test_dataloader**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lRBfZc9dIBEH"
      },
      "outputs": [],
      "source": [
        "# Now, let's define Our test function\n",
        "def test(dataloader, model, loss_fn,epoch,val_losses,val_acc):\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    val_loss, correct = 0, 0\n",
        "    for Image, Label in tqdm(dataloader):\n",
        "\n",
        "\n",
        "       Image=Image.to(device)\n",
        "       Label=Label.to(device)\n",
        "\n",
        "       pred=model(Image)\n",
        "       loss=loss_fn(pred,Label)\n",
        "\n",
        "       pred_labels = pred.argmax(dim=1)\n",
        "\n",
        "       val_loss += loss.item()\n",
        "       correct += (pred.argmax(1) == Label).type(torch.float).sum().item()\n",
        "\n",
        "    # Average loss and accuracy for the epoch\n",
        "    val_loss /= len(dataloader)\n",
        "    val_accuracy= (100*correct/len(dataloader.dataset))\n",
        "    val_losses.append(val_loss)\n",
        "    val_acc.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} \\n Test Accuracy: {val_accuracy:>0.1f}%, Test loss: {val_loss:>7f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi3WZGI3IE54"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "Now, we need to define our loss function and optimizer and start training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anpUP4PtIG7C"
      },
      "outputs": [],
      "source": [
        "# Define our learning rate, loss function and optimizer\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "patch_size = 4\n",
        "n_classes = 10\n",
        "\n",
        "model = ViT(img_size=train_images.shape[2],\n",
        "            in_channels=train_images.shape[1],\n",
        "            num_classes = num_classes,\n",
        "            patch_size=patch_size,\n",
        "            hidden_size=128,\n",
        "            num_layers=8,\n",
        "            num_heads=8).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                    T_max=epochs,\n",
        "                                                    eta_min=0)\n",
        "train_losses, train_acc = [], []\n",
        "val_losses, val_acc = [], []\n",
        "\n",
        "# Let's start training:\n",
        "for epoch in range(epochs):\n",
        "    train(train_loader, model, loss_fn, optimizer,epoch,train_losses,train_acc)\n",
        "    test(test_loader, model, loss_fn,epoch, val_losses,val_acc)\n",
        "    lr_scheduler.step()\n",
        "print(\"Finished!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdZKXDLR79LG"
      },
      "source": [
        "#Inference and plotting the learning curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEoAj4Ko1LNG"
      },
      "source": [
        "Let's see how model performs on the first 10 samples in the test dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoeCd5pD1JGI"
      },
      "outputs": [],
      "source": [
        "# Save our model parameters\n",
        "if not os.path.exists('model'):\n",
        "    os.makedirs('model')\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"model/VIT_Model.pth\")\n",
        "print(\"Saved PyTorch Model State to model/VIT_Model.pth\")\n",
        "\n",
        "# Load the saved model parameters into a new instance of the model\n",
        "model = ViT(img_size=train_images.shape[2],\n",
        "            in_channels=train_images.shape[1],\n",
        "            num_classes = num_classes,\n",
        "            patch_size=patch_size,\n",
        "            hidden_size=128,\n",
        "            num_layers=8,\n",
        "            num_heads=8).to(device)\n",
        "model.load_state_dict(torch.load(\"model/VIT_Model.pth\"))\n",
        "\n",
        "model.eval()\n",
        "for i in range(10):\n",
        "    x, y = test_dataset[i][0], test_dataset[i][1]\n",
        "    x = x.to(device)\n",
        "\n",
        "    pred = model(x.unsqueeze(0))\n",
        "\n",
        "    # for predicting 10 classes\n",
        "    predicted, actual = pred[0].argmax(0).item(), y\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkpiURM17ImG"
      },
      "source": [
        "Let's plot the learning curves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb-WtdUUKAv5"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot the learning curve for loss\n",
        "ax1.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
        "ax1.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Loss Curve')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot the learning curve for accuracy\n",
        "ax2.plot(range(1, epochs + 1), train_acc, label='Training Accuracy')\n",
        "ax2.plot(range(1, epochs + 1), val_acc, label='Validation Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Accuracy Curve')\n",
        "ax2.legend()\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_CLTNYx7OIo"
      },
      "source": [
        "# Positional Embeddings\n",
        "\n",
        "Positional embeddings in ViT are typically initialized randomly, with no clear pattern or relationship at first. As the model trains, these embeddings evolve to encode meaningful positional information.\n",
        "\n",
        "In this part of the assignment, you'll visualize how this evolves by examining the cosine similarity between positional embeddings before and after training. Here's what you need to do:\n",
        "\n",
        "\n",
        "\n",
        "*   Extract positional embeddings from the ViT model before and after training.\n",
        "*   Compute the cosine similarity between each pair of embeddings to observe how similar each patch's position is to others.\n",
        "*   Reshape and visualize these similarities in a grid, creating a similarity heatmap for each patch.\n",
        "*   Use **torch.nn.functional.cosine_similarity** for the calculations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_nOIS37No9c"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "# Your code here\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-1axdXHNUzV"
      },
      "source": [
        "# Attention Map\n",
        "\n",
        "Attention in Transformers allows the model to focus on different parts of the input, assigning weights that reflect the importance of each patch relative to others. In Vision Transformers (ViTs), attention helps capture spatial relationships by highlighting key areas of an image that are crucial for prediction. As the model processes deeper layers, these attention maps become more refined, showing which patches the model relies on to make decisions. Visualizing these attention maps provides insight into how the model interprets and focuses on different regions of the image during inference.\n",
        "\n",
        "In this part of the assignment, you'll then visualize how attention evolves across patches. For this task, you need to:\n",
        "\n",
        "*    Modify the TransformerBlock and ViT model to return attention weights during the forward pass. (Adjust the train and test functions accordingly.)\n",
        "*    Average attentions across head\n",
        "*    Exclude the class token from the attention map for visualization.\n",
        "*   Use **torch.nn.functional.interpolate** to resize the attention map to match the image size for better interpretability.\n",
        "*   Use matplotlib or any other library of your choice to visualize the attention maps.\n",
        "*   Visualize attention map for test_dataset[50] and test_dataset[160]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeAXDVuINrE8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "# Your code here\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9mvmMU1DtwU"
      },
      "source": [
        "# Acknowledgment\n",
        "\n",
        "This Assignment is based on:\n",
        "\n",
        "*   A Course provided by [IBM](https://cognitiveclass.ai/courses/vision-transformers-for-image-classification-hands-on)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
